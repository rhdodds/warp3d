%
%
\input{common_preamble.tex}
%
%----------------------------------------------
%
\usepackage{fancyhdr} \pagestyle{fancy}
\setlength\headheight{15pt}
\lhead{\small{User's Guide - \textit{WARP3D}}}
\rhead{\small{\textit{Parallel Execution}}}
\fancyfoot[L] {\small{\textit{Chapter {\thechapter}}\ \   (Updated: 5-10-2013)}}
\fancyfoot[C] {\small{\thesection-\thepage}}
\fancyfoot[R] {\small{\textit{Solutions with Domain Decomposition}}}
%
%-------------------------------------
\newcounter{sectrefs}
\setcounter{sectrefs}{0}
\setcounter{chapter}{7}
\setcounter{section}{2}
%
%
\begin{document}
%
\section{Solutions with Domain Decomposition: Linux Only}
\noindent
The hybrid version (MPI + threads) of WARP3D is designed to use
domain decomposition for large models analyzed on computational servers/clusters
and supercomputers. The linear
equation solver is \ti{hypre} which runs distributed across
the MPI ranks. WARP3D also performs stiffness assembly with a distributed approach
on the MPI ranks  \footnote[1]{Models with the tie-mesh and/or multi-point constraint
feature prevent use of the distributed assembly. Element stiffness matrices are copied to
rank 0, equations assembled, constraints imposed and equations re-distributed
to ranks for hypre equation solving.}. The additional complexities in model definition
with domain decomposition are often offset by the ability to run on a wider range
of cluster hardware and often with quite efficient parallel execution for
increasing MPI ranks that match available machine resources.

This approach relies of the concept of 
domain decomposition of the finite element model to partition the large data 
structures needed during analysis across the computers performing the solution. 
Domain decomposition assigns each finite element of the model to a domain. 
In our implementation, there are as many domains as MPI ranks used for parallel 
execution. Each rank owns all the data arrays associated 
with the elements assigned to the domain and data for all nodes that lie completely 
within the domain, \ie\ nodes not lying along boundaries shared with other domains. Data for 
nodes on the domain boundaries are shared among the corresponding domains. 

Consider, for example, a nonlinear finite element model that contains 640,000 
elements representing a mixture of 8-node bricks, 20-node bricks, elements 
to make transitions between them, and various nonlinear material models 
assigned to the elements. Suppose the model is analyzed using 32 ranks each 
running WARP3D in parallel (usually on 32 different processor cores). A simple domain 
decomposition strategy assigns 20,000 elements to each of 32 domains for processing. 
However, this simple strategy generally leads to poor load balancing among the 
processors. Domains containing many 20-node elements, for example, may require 
much greater cpu time to complete element stiffness updates. The 
analysis cannot proceed with solution of the equilibrium equations until all 
element stiffnesses are updated. As a consequence, other processes which perform 
computations for domains of 8-node elements become idle, doing no useful work, and 
waiting to begin the equation solving phase. Idle processors quickly degrade parallel 
efficiency of the code. To improve load balance during the analysis, domains 
should be created with different numbers of elements and types 
of elements to achieve a more uniform level 
of computational effort required to process each domain.

In summary, the finite element model consists of domains (1 domain per MPI rank) 
with domains made of blocks of variable size (maximum size set by user) and blocks 
composed of elements of the same type, the same material model, etc. MPI ranks
provide the highest-level parallel execution (in some cases across computers on a network) 
while threads provide parallel execution of blocks on each MPI ranks using the local 
shared-memory.

\subsection{Commands to Start Execution}

\nin These instructions assume the use of Intel MPI Library version 4.x. Other
implementations of MPI may work equally as well.
The steps are:

\begin{offsetpar}{0.3in}
\small
\squishlist
\item
Set variables defining the location for the WARP3D system installation and
for the location of the hybrid executable.
\item
Set environment variables for the number of threads to be
used during execution:\\ (1) \verb,NUM_WARP3D_RANKS,. Number 
of MPI ranks that
will execute WARP3D. The manager process runs on rank 0. \\
(2) \verb,OMP_NUM_THREADS,. Number of threads
used by WARP3D on each MPI rank to process element blocks; recommend
to set \verb,MKL_NUM_THREADS=1,
\item
Increase the runtime stack size for each rank to about 100MB to accommodate increased
size of some arrays for very large models. Unit specified is kbytes..
\item
Set three environment variables used by the MPI runtime
system
\item
Echo various setup values to stdout before starting execution
\item
Start WARP3D execution via the \verb,mpirun, command with
various options. Here, we indicate shared memory
MPI communication .
\squishend
\end{offsetpar}
\normalsize

\nin The following statements extracted from a Bash shell script perform the
steps described above. The complete Bash shell
script is included in the WARP3D distribution (file name:
\small \verb,warp3d_script_linux_hybrid,\normalsize). This script is sufficient to run
on a laptop, desktop on single node of a cluster. The script will require localization
commands to run on specific cluster.s

\footnotesize
\begin{verbatim}
 # 
 #           
 export WARP3D_HOME=/Users/jsmith/warp3d_distribution 
 export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$WARP3D_HOME/linux_packages/lib
 # 
 warp3d_exe="$WARP3D_HOME/run_linux_em64t/warp3d.mpi_omp"
 #
 #     
 export NUM_WARP3D_RANKS=#
 export OMP_NUM_THREADS=#
 export MKL_NUM_THREADS=#     (recommend set same as OMP_NUM_THREADS)
 #
 #           increase the allowable size of the runtime stack. needed
 #           for MPI jobs that also use threads. here set to about 100MB
 #
 ulimit -s 100000
 #
 export MPI_TYPE_MAX=4096
 export I_MPI_PIN_DOMAIN=omp
 export I_MPI_PIN_HOST=1
 #
 echo " Starting WARP3D with MPI + OpenMP hybrid execution..."
 echo "    o Number WARP3D MPI processes @ startup:     " $NUM_WARP3D_RANKS
 echo "    o Number OpenMP threads each WARP3D rank:    " $OMP_NUM_THREADS
 echo "    o Number MKL library threads for hypre rank: " $MKL_NUM_THREADS
 echo " " 
 #
 #
 mpirun -np $NUM_WARP3D_RANKS $WARP3D_HOME/run_linux_em64t/warp3d.mpi_omp
 #
\end{verbatim}
\normalsize
\nin These statements can be enclosed in a Bash shell. Input and output files for WARP3D
can be passed through as shell parameters, and the shell can be executed in background
mode if desired or included with a job scheduling system at a supercomputer center.


\subsection{Extensions to Blocking Command}
To provide the element assignment into domains and blocks, the extended blocking 
command described in Section 2.6 also includes the domain number for 
each element block. The command has the form

\small
\begin{verbatim}
  blocking 
    <block:integer> <block size:integer> <first element in block:integer > ...
                       <domain:integer>
\end{verbatim}

\normalsize
\nin Consider this simple example. The model 
has 2 domains (for analysis using 
2-way MPI execution). There are 5 blocks of elements and we assign blocks 1-3 to 
domain 0 and blocks 4, 5 to domain 1 (based on an estimate of the 
relative workload of the elements appearing 
in the blocks). The blocking command then becomes for this simple example 

\small
\begin{verbatim}    
   blocking       
       1 128   1 0      
       2 128 129 0
       3  64 257 0
       4  64 321 1
       5  20 385 1
\end{verbatim}
\normalsize

\nin The numbering of domains starting with zero maintains consistency between 
the domain number and the MPI rank responsible for all computations on the domain. 
The rules detailed in Section 2.6 for the assignment of elements to blocks carries over 
into MPI-based parallel execution. Generally, all elements in a block must 
be the same type, \eg 8-node, the same type of nonlinearity, the same material model, etc. 
However, all of the blocks assigned to a domain are not
required to have the same type of elements.

\subsection{Extensions of \ti{patwarp} to Support Domain Decomposition}
\nin One task of the \ti{patwarp} program (provided as a component of the WARP3D distribution) 
assigns all elements in the model to blocks. \ti{patwarp} reads the model definition from a Patran 
compatible neutral file (2.5 neutral file format) for the finite element model. The analyst 
specifies to \ti{patwarp} the maximum number of elements in a block and the option of 
vectorized or scalar assignment of elements to blocks. A typical maximum block size 
is 128. The vectorized blocking insures that no elements within a single 
block share a common node. The simpler, scalar blocking eliminates this restriction 
but insures that all elements in a block are the same type, same material model, etc.

The \ti{patwarp} program also: (1) generates an optimized partitioning of finite elements into 
the analyst specified number of domains and, (2) assigns all elements in each domain 
into blocks following the established rules described in Section 2.6. The assignment of 
elements to domains may be accomplished in many ways. An optimal domain 
decomposition is one which: (1) leads to good load balancing among processors during 
execution and, (2) minimizes the communication of nodal information between domains 
for their shared nodes. To achieve (1), \ti{patwarp} internally assigns a work factor for 
each type of element in the model, \ie a 20-node element has a work factor 6.3 times 
larger than the value for an 8-node element. To minimize communication of data 
between domains, the domains are constructed to minimize the number of shared nodes. 
Viewed another way, we decompose a solid into $n$ subvolumes (domains) of various 
shape/size which minimizes the total surface area of the subvolumes. The satisfaction 
of both requirements (2) and (1) simultaneously for very large finite element models 
poses a complex optimization problem. Considerable research has been conducted on 
this specific problem and several efficient optimization algorithms have evolved. 
We have incorporated the METIS software into \ti{patwarp} to perform these 
operations required for the domain decomposition. METIS is developed and 
distributed by Karypis and colleagues at the University of Minnesota Center 
for High Performance Computing.

In summary, the following steps are required by the user to prepare a WARP3D 
input file for parallel execution using the domain decomposition approach:

\begin{offsetpar}{0.3in}
\small
\squishlist
\item
construct the 3-D finite element model using various mesh generators (PATRAN, 
FEMAP, locally developed programs for special models, etc.) which generate a 
Patran compatible neutral file (2.5 format) for the model.
\item
run \ti{patwarp} to process the Patran neutral file for the model. \ti{patwarp} requests 
that the user specify the required number of domains and, as previously, 
the maximum number of elements in a block, and the type (scalar, vector) of 
blocking desired The interactive \ti{patwarp} program prompts the user for the 
required/recommended data.
\item
\ti{patwarp} converts 8-node elements into various transition elements if so 
requested by the user, \ie 12-node elements to connect 8 and 20-node elements.
\item
\ti{patwarp} writes a WARP3D input file with the assignment of each element 
block to a domain using the new form of the blocking command.
\squishend
\end{offsetpar}
\normalsize
Existing WARP3D input files remain compatible with the extended form 
of the blocking command. If no domain number appears in the blocking 
input data, WARP3D input routines assign all blocks to domains in a 
simple round-robin manner based on the number of processors specified on 
the \verb,mpirun, (or similar) command used to start execution. 
This arbitrary domain decomposition may lead to poor load balancing and high 
communication overhead. When the program executes with 1 processor, 
all blocks reside in domain 0, MPI rank 0.

Similarly, WARP3D input files originally constructed with domains for hybrid
execution may be processed with the threaded-only version of the code. 
The threads-only version simply ignores the domain 
numbers in blocking data.

\subsection {Manager-Worker Parallel Architecture}
In parallel finite element codes based on MPI and domain decomposition,
various software architectures have evolved to 
drive the assignment and interchange of data among ranks and to control 
the order and type of computational events. WARP3D adopts the so-called 
manager-worker architecture. In this approach, the manager process (\ie rank 0) 
handles the reading of input data from the user’s files while the other (worker) 
processes enter a wait state. Once the input is read, the manager broadcasts 
a copy of all key information about the model to each of the workers. Examples of 
such data include the nodal coordinates, element incidences, element properties, 
constraints, material properties and other data structures derived directly from 
the input, \eg the list of elements connected to each node in the model. The workers
then independently create all the data structures necessary to store the 
information about elements in the corresponding domain. Thus, for example, 
the total memory allocated to store element stiffnesses is partitioned among 
the ranks; each rank allocates sufficient memory to store only the elements 
in that domain.

The manager maintains control over the sequencing of computational 
tasks to accomplish the nonlinear solution. Workers are sent instructions 
requesting that they perform some part of the solution process for the elements 
in their domain. Examples of such tasks include:
\begin{offsetpar}{0.3in}
\small
\squishlist
\item
update the tangent stiffness 
matrix for the elements, 
\item
compute incremental strains, 
\item
update stresses, 
\item
compute internal forces given the updated displacements and stresses, 
\item
perform operations that form the hypre equation solver, 
\item
compute contributions to the $J$-integral for elements in a domain. 
\squishend
\end{offsetpar}
\normalsize
\nin The manager 
synchronizes the global solution at the end of each Newton iteration of each 
load-time step. For example, the manager gathers the internal forces computed 
at the nodes in each domain from the workers. Internal forces at the 
nodes of a domain not shared with other domains already have their final values. For 
nodes shared with other domains, the workers send their partial contribution 
to the manager. The manager sums the partial internal force contributions 
for shared nodes, then performs checks for convergence of the Newton iterations 
in a load step. The manager handles all logic of the various options in 
the nonlinear solution procedure, \eg\ extrapolation of displacement increments and 
decisions about adaptive solutions. Computation of the incremental vector of 
applied forces for the load step is also accomplished by the manager 
(the computational effort is small relative to the communication effort 
required for parallel computations).

\subsection{Generation of \ti{patran} Result Files} 
By having data for elements in a domain distributed to the MPI rank that 
performs computations for the domain, the generation of \ti{patran} compatible 
result files becomes more complex. When the user requests output of a \ti{patran} 
result file, the manager and workers each write their own part of the \ti{patran} 
file to a partial (fragment) of the final \ti{patran} file. For each output command 
requesting a \ti{patran} result file, this approach generates a series of 
partial \ti{patran} files with one partial file for each domain of the model. 

Consider an analysis running in 32-way parallel with MPI. With an output command, 
the user requests generation of \ti{patran} nodal stresses after step 351 in binary 
format. In an execution without domains, this file is named wnbs0351, 
\ie warp3d nodal binary stresses for step 351. 
With domains, the 32 MPI ranks each write a file where the file name includes the
domain/rank number:  
wnbs0351.0000, wnbs0351.0001, wnbs0351.0002, ... wnbs0351.0031. 
The 4-digit domain number defines the suffix for each file. 
The same strategy is used for element result files. 
For files of nodal displacements, velocities and accelerations, 
the manager (domain 0) has access to all model values 
and simply writes a single \ti{patran} result file without a domain number suffix.

Before \ti{patran} can access results for post-processing,
all the partial files must be combined into 
a single result file for each load step of interest. We provide a support 
program named \verb,pat_combine, to perform this task outside of WARP3D. 
When executed, \verb,pat_combine, examines the current directory for 
the presence of partial result files, combines them as necessary 
(includes computation of nodal averages), and writes a new single result file, \ie
 wnbs0351 in the present example. The partial result files 
remain in the directory (in the present version of \verb,pat_combine,). The 
execution of WARP3D and \verb,pat_combine, may be overlapped for analyses
with long runtimes, \ie once WARP3D writes the partial result 
files for a step, \verb,pat_combine, may be executed to combine 
the result files while WARP3D continues the analysis of subsequent load steps.
%-----------------------------
\subsection{	Generation of Restart Files}
By having data for elements in a domain distributed to the rank that 
performs computations for the domain, the generation of analysis restart files 
also potentially becomes much more complex. At present in WARP3D, 
the workers send all element strain and stress values to the manager 
when the user requests generation of a restart restart file. 

This approach generates a single (binary) restart file, 
as is the case for threaded-only execution of WARP3D. 
Unfortunately, this strategy has two disadvantages: 
(1) it generates a significant communication load 
to send the large volumes of element data to the manager, and 
(2) the manager must allocate sufficient memory to store the results 
for all elements in the model. A similar communication cost and 
memory utilization overhead occurs during restart when the process 
is reversed. Consequently, we strongly recommend generating 
restart files only after critical load steps.

Several improved forms of this I/O strategy for restart are under consideration. 

\subsection{	Summary of Parallel Tasks with Domain Decomposition}
This section has described current capabilities of 
WARP3D to execute computational tasks in parallel using a domain decomposition approach
with a hybrid version of the code
(MPI + threads). The table here summarizes more completely the status of 
various solution tasks executing in parallel. 
%
%=================================================
\begin{table}[htb]	
\setlength{\extrarowheight}{4.0pt}
\footnotesize
\centering
{
\begin{tabular}{| p{3.0in} |p{0.6in} |p{0.6in} |p{0.7in} |}
\hline
\tb{Computational Task} & \tb{Master Rank} & \tb{Worker Rank}
 & \tb{Thread Processing of Blocks} \\
\hline \hline
\tb{Initialization:} & & &\\ \hline
\hspace{0.1in}reading input file & \hspace{0.2in} x & & \\ \hline
\hspace{0.1in}setup of shared data structures & \hspace{0.2in} x & & \\ \hline
\tb{Start of New Load Step} & & & \\ \hline
\hspace{0.1in}compute equivalent nodal loads & \hspace{0.2in} x& &  \\ \hline
\hspace{0.1in}step size reduction (for crack growth) & \hspace{0.2in} x & & \\ \hline
\hspace{0.1in}crack growth evaluation & \hspace{0.2in} x & & \\ \hline
\hspace{0.1in}computation of effective load increment & \hspace{0.2in} x &  & \\ \hline
\tb{Newton Iterations for Step} & & &\\ \hline
\hspace{0.1in}element stiffness update & \hspace{0.2in} x & \hspace{0.2in} x & \hspace{0.2in} x \\ \hline
\hspace{0.1in}element mass matrices & \hspace{0.2in} x & \hspace{0.2in} x & \hspace{0.2in} x \\ \hline
\hspace{0.1in}Assembly of structure stiffness (for hypre) & \hspace{0.2in} x &  \hspace{0.2in} x & \hspace{0.2in} x\\ \hline
\hspace{0.1in}linear eq. solver: hypre (Pardiso not available) & \hspace{0.2in} x & \hspace{0.2in} x & \hspace{0.2in} x \\ \hline
\hspace{0.1in}element strain update & \hspace{0.2in} x & \hspace{0.2in} x & \hspace{0.2in} x \\ \hline
\hspace{0.1in}element stress update & \hspace{0.2in} x & \hspace{0.2in} x & \hspace{0.2in} x \\ \hline
\hspace{0.1in}element internal force update & \hspace{0.2in} x & \hspace{0.2in} x & \hspace{0.2in} x \\ \hline
\hspace{0.1in}contact detection/computation of contact forces & \hspace{0.2in} x & \hspace{0.2in} x  & \\ \hline
\hspace{0.1in}assembly of global residual force vector & \hspace{0.2in} x & & \\ \hline
\hspace{0.1in}convergence tests & \hspace{0.2in} x & & \\ \hline
\tb{\ti{J}-Integral Computation } & & &\\ \hline
\hspace{0.1in}input and setup of domains, error checks & \hspace{0.2in} x  & & \\ \hline
\hspace{0.1in}expansion of automatic domains & \hspace{0.2in} x  & & \\ \hline
\hspace{0.1in}$J$ calculation over a domain & \hspace{0.2in} x & \hspace{0.2in} x  &\\ \hline
\hspace{0.1in}output of values & \hspace{0.2in} x & &  \\ \hline
\tb{Output} & & & \\ \hline
\hspace{0.1in}translation of user output commands & \hspace{0.2in} x  & & \\ \hline
\hspace{0.1in}output of \ti{patran} compatible files & \hspace{0.2in} x & \hspace{0.2in} x &  \\ \hline
\end{tabular}
}	
\normalsize
%
\caption{\small Table: Summary of Tasks Executed in Parallel with Domain Decomposition.
\normalsize}
\end{table}



\end{document}
